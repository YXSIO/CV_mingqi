## Week 8

### 模型的评估与部署

### 学习率的调整
理论上，如果将学习率调大10倍，现在10次训练就可以达成之前100次的训练效果。
一般使用工具默认的学习率，如果收敛太慢，比如训练了十几个小时，在训练集和验证集上仍在收敛，则可尝试将学习率加大几倍，不要一下调成太大。
如果误差波动过大，无法收敛，则可考虑减小学习率，以便微调模型。
在测试阶段建议使用较大的学习率，在短时间内测算过拟合位置，尤其好用。
在预训练模型的基础上fine-tune模型时，一般使用较小的学习率；反之，如果直接训练，则使用较大的学习率。
对于不同层可使用不同学习率，比如可对新添加的层使用较大的学习率，或者“冻住”某些层。


### 参数初始化
参数初始化又称为权重初始化（weight initialization）或权值初始化。深度学习模型训练过程的本质是对weight（即参数 W）进行更新，这需要每个参数有相应的初始值。说白了，神经网络其实就是对权重参数w不停地迭代更新，以达到较好的性能。

模型权重的初始化对于网络的训练很重要，不好的初始化参数会导致梯度传播问题，降低训练速度；而好的初始化参数能够加速收敛，并且更可能找到较优解。如果权重一开始很小，信号到达最后也会很小；如果权重一开始很大，信号到达最后也会很大。


### 优化器
Pytorch提供在迭代过程中修改学习率的方法。最简单的方法是手动修改学习率的值。优化器optimizer通过param_group提供对不同层使用不同的优化方法，其中每组参数保存了各自的学习率、动量等，如果只设置了一种优化方法，修改其第0组的lr即可。

1. 有序调整
	按一定规则调整，比如使用余弦退火(CosineAnnealing)，指数衰减(Exponential)，或者步长(Step)等事先定制的规则调整学习率。
2. 自适应调整
	通过监测某个指标的变化情况(loss、accuracy)，当指标不再变好时，调整学习率 (ReduceLROnPlateau);
3. 自定义调整
	使用自定义的lambda函数调整学习率(LambdaLR)

### 损失函数
1. 损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

2. 损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。








 

