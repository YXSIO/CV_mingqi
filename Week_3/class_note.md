## Two stage detection: Faster RCNN - end 2 end 
### Improvement
	1. RPN: replace the selective search to generate ROI.
		-Classification:  
			1. The aspect reatio of the anchor is generated by K-mean.
			2. If any anchor is out side of the image, simply mute the negative values at 0.
			3. The shape of the anchor box is defined by (x1, y1, x2, y2) or (x_mid, y_mid, w, h)
		-Regression:  
			1. Offset regression: p-a -> g-a; 
			2. Encoder: find the difference between and a.
			3. Decoder: from the difference between p and a, find p. 
		-End to End training and two stages
			1. Two stages: classic method, RPN->RCNN->RPN->RCNN, which is more accurate but very mannual.
			2. End2End: the loss of RPN and the loss of RCNN is combined.


## One stage: why we have to train proposal first?
	1. SSD-single short detection is the first version of one stage detection
	2. YOLO V1: 
		-Divide the whole image into cells.
		-One cell will be responsible for predicting an object as long as the object's center locates in that cell. 
		-Each cell predict B bounding box for the same(single) object and the confidence = probability * IOU
		-The output for V1 is 7 X 7 X (5 X B + C): 5 = probability + coorinate
		-The loss function is consisted of three parts: bounding box regresssion + confidence + class probability
			1. training confidence = P(object or not) * IOU(with groud truth)
			2. testing confidence = P(cls|obj) * P(obj) * IOU
		-Understand how sqrt and log helps to normalize between big and small bounding boxes. 
		-Pros: fast
		-Cons: 
			1. not very good at small objects. To improve, we can have finer cells or have more points in the feature map for RCNN. 
			2. Bad for crowed objects, since each cell only predict one object.
	3. YOLO V2:
		-Add BN
		-Finer grids: 13 X 13
		-Structure perspective: V2 add short cut to combine the physical info(edge and corner) and semantic info together. For a detection problem, it can be decomposed into regression(location) and cls(semantics). 
		-Use reorg to merge shallow layers with deep layers
		-Anchor in V2: [0.65, 0.34.....] 10 numbers defines five defines 5 anchor. Each anchor is defined by its width and height. num_i/W_pic * 13.
		-Multi-scale training to accommodate image with different sizes. 
	4. YOLO V3:
		-Modularize the NN structure
		-Multi scale structure: three output exists in the NN with different feature map resolutions. e.g. ori image: 256 X 256, feature map 32 * 32, then each pixel in the feature maps represent 8 pixel in the original image.
		-Softmax classification -> logistic regression (one vs all). The advantage is that for softmax only output one class and the peak will supress others. With one vs all LR on many different classes, we get an classificaiton for each class independently.
		-Mapping the anchor onto each cells. (four steps project back and forth to [0,1])

		
## Compare faster RCNN with Yolo
	1. RPN: 38*50 each pixel correspond to 9 anchors. While YOLO, the anchor is on the cell level instead of convoluted point level.
	2. Stage number is different


Question: 
1. confidence = P(object or not) * IOU(with groud truth). How c is calculated? The defination of C_hat is different in trainig and testing?
2. Multi scale training: 320, 352 是对什么的描述？为什么不直接resize？
3. How reorg is different from Pooling






